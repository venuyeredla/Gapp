# AI Maths
	
    The function which is to be optimized is called objective function. It also may be called as cost function, loss function or error function. 

Local minimum and local maximum .

Global minimum and global maximum. 

Gradient descent is a technique for maximization of the function. Reducing f(x) moving x in small 
steps with opposite sign of derivative. 

Sometimes we need to find all derivatives of a function whose input and output are both vectors. 
The matrix which contains all such partial vectors are known Jacobian matrix. 